#!/usr/bin/env python3\n\"\"\"\nTests for Analytics Infrastructure\n\nComprehensive tests for the analytics infrastructure including\nmetrics collection, event tracking, alerting, and performance monitoring.\n\"\"\"\n\nimport asyncio\nimport pytest\nimport tempfile\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom unittest.mock import Mock, patch\n\nfrom xencode.analytics.analytics_infrastructure import (\n    AnalyticsInfrastructure,\n    AnalyticsInfrastructureConfig,\n    AnalyticsLevel,\n    AnalyticsAlert\n)\nfrom xencode.analytics.event_tracker import EventCategory\n\n\nclass TestAnalyticsInfrastructure:\n    \"\"\"Test suite for analytics infrastructure\"\"\"\n    \n    @pytest.fixture\n    def temp_storage(self):\n        \"\"\"Create temporary storage directory\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            yield Path(temp_dir)\n    \n    @pytest.fixture\n    def config(self, temp_storage):\n        \"\"\"Create test configuration\"\"\"\n        return AnalyticsInfrastructureConfig(\n            level=AnalyticsLevel.COMPREHENSIVE,\n            enable_prometheus=False,  # Disable for testing\n            enable_system_monitoring=True,\n            storage_path=temp_storage,\n            metrics_collection_interval=1,  # Fast for testing\n            analytics_aggregation_interval=1\n        )\n    \n    @pytest.fixture\n    def analytics_infrastructure(self, config):\n        \"\"\"Create analytics infrastructure instance\"\"\"\n        return AnalyticsInfrastructure(config)\n    \n    @pytest.mark.asyncio\n    async def test_initialization(self, analytics_infrastructure):\n        \"\"\"Test analytics infrastructure initialization\"\"\"\n        assert analytics_infrastructure.config.level == AnalyticsLevel.COMPREHENSIVE\n        assert analytics_infrastructure.analytics_engine is not None\n        assert analytics_infrastructure.metrics_collector is not None\n        assert analytics_infrastructure.event_tracker is not None\n        assert not analytics_infrastructure._running\n    \n    @pytest.mark.asyncio\n    async def test_start_stop(self, analytics_infrastructure):\n        \"\"\"Test starting and stopping analytics infrastructure\"\"\"\n        # Start\n        await analytics_infrastructure.start()\n        assert analytics_infrastructure._running\n        \n        # Stop\n        await analytics_infrastructure.stop()\n        assert not analytics_infrastructure._running\n    \n    @pytest.mark.asyncio\n    async def test_user_action_tracking(self, analytics_infrastructure):\n        \"\"\"Test user action tracking\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Track user action\n            event_id = analytics_infrastructure.track_user_action(\n                \"file_opened\",\n                \"user123\",\n                filename=\"test.py\",\n                file_size=1024\n            )\n            \n            assert event_id is not None\n            \n            # Verify event was tracked\n            events = analytics_infrastructure.analytics_engine.get_recent_events(hours=1)\n            user_events = [e for e in events if e.context.user_id == \"user123\"]\n            assert len(user_events) > 0\n            \n            event = user_events[0]\n            assert event.event_type == \"file_opened\"\n            assert event.properties[\"filename\"] == \"test.py\"\n            assert event.properties[\"file_size\"] == 1024\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_performance_tracking(self, analytics_infrastructure):\n        \"\"\"Test performance tracking\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Track performance event\n            event_id = analytics_infrastructure.track_performance(\n                \"document_processing\",\n                150.5,  # 150.5ms\n                \"document_processor\",\n                document_type=\"pdf\",\n                pages=10\n            )\n            \n            assert event_id is not None\n            \n            # Verify performance baseline was updated\n            assert \"document_processing\" in analytics_infrastructure.performance_baselines\n            baseline = analytics_infrastructure.performance_baselines[\"document_processing\"]\n            assert len(baseline[\"samples\"]) > 0\n            assert baseline[\"baseline_ms\"] > 0\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_error_tracking(self, analytics_infrastructure):\n        \"\"\"Test error tracking\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Track error event\n            event_id = analytics_infrastructure.track_error(\n                \"validation_error\",\n                \"Invalid file format\",\n                \"document_processor\",\n                filename=\"invalid.txt\"\n            )\n            \n            assert event_id is not None\n            \n            # Verify error was tracked\n            events = analytics_infrastructure.analytics_engine.get_recent_events(hours=1)\n            error_events = [e for e in events if e.category == EventCategory.ERROR]\n            assert len(error_events) > 0\n            \n            error_event = error_events[0]\n            assert error_event.event_type == \"validation_error\"\n            assert error_event.properties[\"error_message\"] == \"Invalid file format\"\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_ai_interaction_tracking(self, analytics_infrastructure):\n        \"\"\"Test AI interaction tracking\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Track AI interaction\n            event_id = analytics_infrastructure.track_ai_interaction(\n                \"gpt-4\",\n                \"code_analysis\",\n                \"user123\",\n                2500.0,  # 2.5 seconds\n                tokens_used=150,\n                success=True\n            )\n            \n            assert event_id is not None\n            \n            # Verify AI interaction was tracked\n            events = analytics_infrastructure.analytics_engine.get_recent_events(hours=1)\n            ai_events = [e for e in events if e.category == EventCategory.AI_INTERACTION]\n            assert len(ai_events) > 0\n            \n            ai_event = ai_events[0]\n            assert ai_event.event_type == \"code_analysis\"\n            assert ai_event.properties[\"model\"] == \"gpt-4\"\n            assert ai_event.properties[\"tokens_used\"] == 150\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_operation_timer(self, analytics_infrastructure):\n        \"\"\"Test operation timer context manager\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Use operation timer\n            with analytics_infrastructure.time_operation(\"test_operation\", \"test_component\"):\n                await asyncio.sleep(0.1)  # Simulate work\n            \n            # Verify performance was tracked\n            events = analytics_infrastructure.analytics_engine.get_recent_events(hours=1)\n            perf_events = [e for e in events if e.category == EventCategory.PERFORMANCE]\n            assert len(perf_events) > 0\n            \n            perf_event = perf_events[0]\n            assert perf_event.event_type == \"test_operation\"\n            assert perf_event.metrics[\"duration_ms\"] >= 100  # At least 100ms\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_alert_generation(self, analytics_infrastructure):\n        \"\"\"Test alert generation\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Generate multiple errors to trigger error spike alert\n            for i in range(15):  # Above threshold of 10\n                analytics_infrastructure.track_error(\n                    \"test_error\",\n                    f\"Test error {i}\",\n                    \"test_component\"\n                )\n            \n            # Wait a bit for alert processing\n            await asyncio.sleep(0.5)\n            \n            # Check for error spike alert\n            active_alerts = analytics_infrastructure.get_active_alerts()\n            error_spike_alerts = [a for a in active_alerts if a.alert_type == \"error_spike\"]\n            assert len(error_spike_alerts) > 0\n            \n            alert = error_spike_alerts[0]\n            assert alert.severity == \"high\"\n            assert \"Error Spike Detected\" in alert.title\n            assert alert.metadata[\"error_count\"] >= 10\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_slow_operation_alert(self, analytics_infrastructure):\n        \"\"\"Test slow operation alert\"\"\"\n        # Set low threshold for testing\n        analytics_infrastructure.config.slow_operation_threshold_ms = 50.0\n        \n        await analytics_infrastructure.start()\n        \n        try:\n            # Track slow operation\n            analytics_infrastructure.track_performance(\n                \"slow_operation\",\n                100.0,  # 100ms, above 50ms threshold\n                \"test_component\"\n            )\n            \n            # Wait a bit for alert processing\n            await asyncio.sleep(0.5)\n            \n            # Check for slow operation alert\n            active_alerts = analytics_infrastructure.get_active_alerts()\n            slow_op_alerts = [a for a in active_alerts if a.alert_type == \"slow_operation\"]\n            assert len(slow_op_alerts) > 0\n            \n            alert = slow_op_alerts[0]\n            assert alert.severity == \"medium\"\n            assert \"Slow Operation Detected\" in alert.title\n            assert alert.metadata[\"duration_ms\"] == 100.0\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_performance_degradation_alert(self, analytics_infrastructure):\n        \"\"\"Test performance degradation alert\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Establish baseline with fast operations\n            for i in range(10):\n                analytics_infrastructure.track_performance(\n                    \"baseline_operation\",\n                    50.0,  # 50ms baseline\n                    \"test_component\"\n                )\n            \n            # Wait for baseline to be established\n            await asyncio.sleep(0.5)\n            \n            # Track significantly slower operation (2x+ slower)\n            analytics_infrastructure.track_performance(\n                \"baseline_operation\",\n                150.0,  # 150ms, 3x slower than baseline\n                \"test_component\"\n            )\n            \n            # Wait for alert processing\n            await asyncio.sleep(0.5)\n            \n            # Check for performance degradation alert\n            active_alerts = analytics_infrastructure.get_active_alerts()\n            perf_alerts = [a for a in active_alerts if a.alert_type == \"performance_degradation\"]\n            assert len(perf_alerts) > 0\n            \n            alert = perf_alerts[0]\n            assert alert.severity == \"medium\"\n            assert \"Performance Degradation Detected\" in alert.title\n            assert alert.metadata[\"degradation_factor\"] >= 2.0\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_alert_resolution(self, analytics_infrastructure):\n        \"\"\"Test alert resolution\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Create alert by tracking slow operation\n            analytics_infrastructure.config.slow_operation_threshold_ms = 50.0\n            analytics_infrastructure.track_performance(\n                \"slow_operation\",\n                100.0,\n                \"test_component\"\n            )\n            \n            await asyncio.sleep(0.5)\n            \n            # Get the alert\n            active_alerts = analytics_infrastructure.get_active_alerts()\n            assert len(active_alerts) > 0\n            \n            alert = active_alerts[0]\n            assert not alert.resolved\n            \n            # Resolve the alert\n            success = analytics_infrastructure.resolve_alert(alert.alert_id)\n            assert success\n            assert alert.resolved\n            assert alert.resolved_at is not None\n            \n            # Verify alert is no longer active\n            active_alerts = analytics_infrastructure.get_active_alerts()\n            assert alert not in active_alerts\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_alert_callbacks(self, analytics_infrastructure):\n        \"\"\"Test alert callbacks\"\"\"\n        callback_called = False\n        received_alert = None\n        \n        def alert_callback(alert: AnalyticsAlert):\n            nonlocal callback_called, received_alert\n            callback_called = True\n            received_alert = alert\n        \n        # Add callback\n        analytics_infrastructure.add_alert_callback(alert_callback)\n        \n        await analytics_infrastructure.start()\n        \n        try:\n            # Trigger alert\n            analytics_infrastructure.config.slow_operation_threshold_ms = 50.0\n            analytics_infrastructure.track_performance(\n                \"slow_operation\",\n                100.0,\n                \"test_component\"\n            )\n            \n            await asyncio.sleep(0.5)\n            \n            # Verify callback was called\n            assert callback_called\n            assert received_alert is not None\n            assert received_alert.alert_type == \"slow_operation\"\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_system_status(self, analytics_infrastructure):\n        \"\"\"Test system status reporting\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Get system status\n            status = analytics_infrastructure.get_system_status()\n            \n            assert \"analytics_infrastructure\" in status\n            assert status[\"analytics_infrastructure\"][\"running\"] == True\n            assert status[\"analytics_infrastructure\"][\"level\"] == AnalyticsLevel.COMPREHENSIVE.value\n            \n            assert \"components\" in status\n            assert \"analytics_engine\" in status[\"components\"]\n            \n            assert \"alerts\" in status\n            assert \"total_alerts\" in status[\"alerts\"]\n            \n            assert \"performance\" in status\n            assert \"baselines\" in status[\"performance\"]\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_analytics_report(self, analytics_infrastructure):\n        \"\"\"Test analytics report generation\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Generate some activity\n            analytics_infrastructure.track_user_action(\"test_action\", \"user123\")\n            analytics_infrastructure.track_performance(\"test_op\", 100.0, \"test_component\")\n            analytics_infrastructure.track_error(\"test_error\", \"Test error\", \"test_component\")\n            \n            await asyncio.sleep(0.5)\n            \n            # Generate report\n            report = analytics_infrastructure.generate_analytics_report(hours=1)\n            \n            assert \"report_generated_at\" in report\n            assert \"time_period_hours\" in report\n            assert report[\"time_period_hours\"] == 1\n            \n            assert \"infrastructure_status\" in report\n            assert \"analytics_engine_report\" in report\n            assert \"alerts\" in report\n            assert \"performance\" in report\n            \n            # Check alert summary\n            alert_summary = report[\"alerts\"]\n            assert \"total_alerts\" in alert_summary\n            assert \"alerts_by_type\" in alert_summary\n            assert \"alerts_by_severity\" in alert_summary\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_performance_summary(self, analytics_infrastructure):\n        \"\"\"Test performance summary\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Track some performance data\n            for i in range(5):\n                analytics_infrastructure.track_performance(\n                    \"test_operation\",\n                    50.0 + i * 10,  # Varying durations\n                    \"test_component\"\n                )\n            \n            await asyncio.sleep(0.5)\n            \n            # Get performance summary\n            summary = analytics_infrastructure.get_performance_summary()\n            \n            assert \"baselines\" in summary\n            assert \"test_operation\" in summary[\"baselines\"]\n            \n            baseline = summary[\"baselines\"][\"test_operation\"]\n            assert baseline[\"sample_count\"] == 5\n            assert baseline[\"baseline_ms\"] > 0\n            \n            assert \"recent_history_count\" in summary\n            assert \"slow_operation_threshold_ms\" in summary\n            \n        finally:\n            await analytics_infrastructure.stop()\n    \n    @pytest.mark.asyncio\n    async def test_state_persistence(self, analytics_infrastructure, temp_storage):\n        \"\"\"Test analytics state persistence\"\"\"\n        await analytics_infrastructure.start()\n        \n        try:\n            # Generate some data\n            analytics_infrastructure.track_performance(\"test_op\", 100.0, \"test_component\")\n            \n            # Trigger alert\n            analytics_infrastructure.config.slow_operation_threshold_ms = 50.0\n            analytics_infrastructure.track_performance(\"slow_op\", 200.0, \"test_component\")\n            \n            await asyncio.sleep(0.5)\n            \n            # Stop (should save state)\n            await analytics_infrastructure.stop()\n            \n            # Check that state file was created\n            state_file = temp_storage / \"analytics_state.json\"\n            assert state_file.exists()\n            \n            # Verify state file content\n            import json\n            with open(state_file) as f:\n                state = json.load(f)\n            \n            assert \"timestamp\" in state\n            assert \"config\" in state\n            assert \"alerts\" in state\n            assert \"performance_baselines\" in state\n            \n        except Exception:\n            # Ensure cleanup even if test fails\n            if analytics_infrastructure._running:\n                await analytics_infrastructure.stop()\n            raise\n\n\nif __name__ == \"__main__\":\n    # Run tests\n    pytest.main([__file__, \"-v\"])\n"